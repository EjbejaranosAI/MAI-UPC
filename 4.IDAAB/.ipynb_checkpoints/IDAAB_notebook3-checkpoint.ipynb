{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "<small><i>Updated January 2022</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style = \"border-radius:10px;border-width:3px;border-color:darkblue;font-family:Verdana,sans-serif;font-size:16px;\">\n",
    "<h2>Outline</h2>\n",
    "<ol>\n",
    "    <li>What is a recommender system?</li>\n",
    "    <li>How to build a recommender system? </li>\n",
    "    <li>How to evaluate its success?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to build a recommender system:\n",
    "<ol>\n",
    "    <li>Data collection and understanding</li>\n",
    "    <li>Data filtering/cleaning</li>\n",
    "    <li>Learning<br>\n",
    "        <span style=\"font-size:smaller\">E.g., using item/user similarity function</span></li>\n",
    "    <li>Evaluation</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Hands on\n",
    "## Evaluating a RecSys: case use on our CF RecSys for Movielens\n",
    "\n",
    "We will use again the MovieLens dataset, which you should have downloaded to complete the first notebook.\n",
    "\n",
    "Let us first load the libraries that we are going to need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autosave 150\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, next, the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The dataset is composed of 3 main files\n",
    "\n",
    "# The users file \n",
    "u_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
    "users = pd.read_csv('ml-100k/u.user', sep='|', names=u_cols)\n",
    "\n",
    "# The movies (items) file\n",
    "m_cols = ['movie_id', 'title', 'release_date']\n",
    "# It contains aditional columns indicating, among other the movies' genre.\n",
    "# Let's only load the first three columns:\n",
    "movies = pd.read_csv('ml-100k/u.item', sep='|', names=m_cols, usecols=range(3), encoding='latin-1')\n",
    "\n",
    "# The ratings file \n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('ml-100k/u.data', sep='\\t', names=r_cols)\n",
    "\n",
    "\n",
    "# We merge all three dataframes into a single dataset\n",
    "data = pd.merge(pd.merge(ratings, users), movies)\n",
    "# and keep only the columns that we are going to use\n",
    "data = data[['user_id', 'rating', 'movie_id', 'title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a subset of just 100 users, the ones with a largest number of ratings. We keep a 20% of them for evaluation purposes, and learn with the remaining 80%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7) # for replicability\n",
    "\n",
    "# We keep only data regarding the 100 users with the largest number of ratings\n",
    "#user_id_most_raters = data.groupby('user_id').size().sort_values(ascending=False).head(100).keys()\n",
    "#data = data[data['user_id'].isin(user_id_most_raters)].copy()\n",
    "data = data[data['user_id']<=100].copy() # get only data from 100 users\n",
    "print('Dataset size:', data.shape)\n",
    "print('Usuaris:', data.user_id.nunique())\n",
    "print('Films:',data.movie_id.nunique())\n",
    "\n",
    "# Make train/test partition\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "trData = data[msk]\n",
    "tsData = data[~msk]\n",
    "\n",
    "print(\"The training set has \"+ str(trData.shape[0]) +\" ratings, and the test set \"+ str(tsData.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the similarity function defined in the second noteboook. By default, we use Pearson Similarity and a lower bound of 5 minimum items rated in common to measure it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# euclidean distance based similarity using scipy's euclidean definition\n",
    "def euclideanSimilarity(v1, v2):\n",
    "    return 1.0 / (1.0 + euclidean(v1,v2))\n",
    "\n",
    "# wrapper for pearson correlation similarity which uses scipy's definition\n",
    "def pearsonSimilarity(v1, v2):\n",
    "    res = pearsonr(v1, v2)[0]\n",
    "    if math.isnan(res) or res < 0:\n",
    "        res = 0\n",
    "    return res\n",
    "\n",
    "# Returns a similarity score for two users\n",
    "def similarityFunction(myData, user1, user2, similarity=pearsonSimilarity, minCommonItems=5):\n",
    "    # Get movies rated by user1\n",
    "    movies_user1 = myData[myData['user_id'] == user1]\n",
    "    # Get movies rated by user2\n",
    "    movies_user2 = myData[myData['user_id'] == user2]\n",
    "    \n",
    "    # Find commonly rated films\n",
    "    rep = pd.merge(movies_user1, movies_user2, on='movie_id')    \n",
    "    if len(rep) < minCommonItems:\n",
    "        return 0   \n",
    "\n",
    "    return similarity(rep['rating_x'], rep['rating_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copy** in the following cell you implementation of the CF class that you implemented in the second notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### TODO: copy here your implementation of CollaborativeFiltering class from Notebook 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train it with the training subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_recsys = CollaborativeFiltering()\n",
    "my_recsys.fit(trData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are interested in defining how we are going to evaluate the recommender systems that we build.\n",
    "\n",
    "## Evaluation criteria: metrics\n",
    "\n",
    "Performance evaluation of recommender systems is itself an entire research topic. Some commonly used metrics include:<br>\n",
    "* $RMSE = \\sqrt{(\\frac{\\sum(\\hat{y}-y)^2}{n})}$\n",
    "* Precision / Recall / F-scores\n",
    "* ROC curves\n",
    "* Cost curves\n",
    "\n",
    "Let's implement the root mean square error (RMSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_pred, y_true):\n",
    "    \"\"\" Compute Root Mean Squared Error. \"\"\"\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\n",
    "\n",
    "def evaluate(estimation_func, myTsData, metric=rmse):\n",
    "    \"\"\" RMSE-based predictive performance evaluation with pandas. \"\"\"\n",
    "    \n",
    "    # we keep the pairs user-movie for which we are going to obtain an estimation\n",
    "    pairs_to_estimate = zip(myTsData.user_id, myTsData.movie_id)\n",
    "\n",
    "    # we do obtain the estimations\n",
    "    estimated_values = np.array([estimation_func(u,i) for (u,i) in pairs_to_estimate ])\n",
    "\n",
    "    # finally, we compare the estimations and the real values with the chosen metric\n",
    "    real_values = myTsData.rating.values\n",
    "    return metric(estimated_values, real_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the RMSE of this estimation procedure within the test data (this might take a while!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE for Collaborative Recommender: %s' % evaluate(my_recsys.predict, tsData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we obtain a performance measure for our RecSys that we can use for evaluation or model selection. In this context, RMSE and MAE (mean absolute error) are the most popular metrics. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question #1.-<br>\n",
    "<span style=\"color:black\">Implement MAE.\n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "def mae(y_pred, y_true):\n",
    "        print(\"mae: not implemented yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE for Collaborative Recommender: %s' % evaluate(my_recsys.predict, tsData, metric=mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the main criticism of these metrics is that they really don't measure user experience. \n",
    "As users are commonly presented with a set of `N` recommendations for they to choose from, evaluating according to the top-N recommendations is probably closer to what is important to the final user.\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question #2.-<br>\n",
    "<span style=\"color:black\">Create a new method in the recommender class that returns the top-N recommendations for a user, where N is a parameter of the method. Recommendations need to be movies unseen by the user.</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "import operator\n",
    "\n",
    "def get_top_N_recommendations(self, user_id, N=10):\n",
    "    print(\"get_top_N_recommendations: not implemented yet\")\n",
    "\n",
    "\n",
    "# We add this function to the CF class: it returns the N items with largest \n",
    "# estimated rating as recommendations\n",
    "CollaborativeFiltering.get_top_N_recommendations = get_top_N_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, now, we can obtain the set of movies recommended to user 'user_id':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_recsys.get_top_N_recommendations(user_id=1,N=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we evaluate this type of recommendation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision at N\n",
    "In machine learning, we define precision as:\n",
    "$$ P=\\frac{TP}{FP+TP}$$\n",
    "that is, the proportion of really relevant items among all the recommended items.\n",
    "\n",
    "Precision at K is defined as the precision calculated by considering only the first K recommendations:\n",
    "$$ P@K=\\frac{TP@K}{K}$$\n",
    "\n",
    "But which is the appropriate `K` value? To deal with this inconvenience, we use the average precision, which summarizes over different values as follows,\n",
    "\n",
    "$$AP@N = \\frac{1}{\\min(N,m)}\\sum_{K=1}^N P@K \\cdot rel(u,k)$$\n",
    "\n",
    "where `m` is the number of really relevant elements, and $rel(u,k)$ is a function that tells whether the `k`-th recommended element is really relevant to user `u` or not.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question #3.-<br>\n",
    "<span style=\"color:black\">Why do we normalize the Average Precision with $\\min(m,N)$?</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[ Your answer here! ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Question #4.-<br>\n",
    "<span style=\"color:black\">Implement AP@N.</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "def APatN(l_pred, l_real, N):\n",
    "    '''Calculate Average Precision at N. Assumes that there is no repeated elements in l_pred, \n",
    "       and l_pred needs to be in descending ordering. '''\n",
    "    AP = 0.0\n",
    "    TP = 0.0\n",
    "\n",
    "    for i,item in enumerate(l_pred):\n",
    "        if item in l_real:\n",
    "            TP += 1\n",
    "            ### TODO: computate Precision@K and add it to AP\n",
    "\n",
    "    return AP / min(len(l_real), N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test the Average Precision at N of the recommendations that we make to an user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u=7\n",
    "N=10\n",
    "l_pred = my_recsys.get_top_N_recommendations(user_id=u,N=N)\n",
    "\n",
    "userLikedMovies = tsData[ tsData['user_id'] == u ] \n",
    "l_real = list(userLikedMovies.movie_id[ userLikedMovies['rating'] > 3 ]) # we assume like if rating > 3\n",
    "\n",
    "print('AP@N (with N=',N,') for user',u,': %s' % APatN(l_pred, l_real, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, so far, we only have considered the case of a single recommendation (for a single user). To average over multiple users, we use the Mean Average Precision:\n",
    "$$MAP@N=\\frac{1}{|U|}\\sum_{u=1}^{|U|}AP_u@N=\\frac{1}{|U|}\\sum_{u=1}^{|U|}\\frac{1}{\\min(N,m)}\\sum_{K=1}^{N}P_u@K\\cdot rel(u,k)$$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Question #5.-<br>\n",
    "<span style=\"color:black\">Implement MAP@N.</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5\n",
    "def MAPatN(lists_pred, lists_real, N):\n",
    "    print(\"get_top_N_recommendations: not implemented yet\")\n",
    "\n",
    "\n",
    "def evaluate_topN(estimation_func, myTsData, metric=MAPatN, N=10):\n",
    "    '''Performance evaluation of Top-N-based recommendations.'''\n",
    "\n",
    "    dfPosScores = myTsData[ myTsData['rating'] > 3 ] # we assume like if rating > 3\n",
    "    lists_real = dfPosScores.groupby('user_id')['movie_id'].apply(list)\n",
    "    lists_real = [ lists_real.iloc[i] for i in range(len(lists_real))]\n",
    "    \n",
    "    # find the users\n",
    "    users_in_tspos = list(dfPosScores.user_id.unique())\n",
    "\n",
    "    # we do obtain the recommendations for all the users\n",
    "    lists_recommendations = [ estimation_func(u,N) for u in users_in_tspos ]\n",
    "\n",
    "    return metric(lists_recommendations, lists_real, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test the CF RecSys when it provides Top-N recommendations (we keep a subsample of the test set; otherwise, this might take so long!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "# select a subset of users\n",
    "users = list(tsData['user_id'].unique())\n",
    "random.shuffle(users)\n",
    "users = users[:int(len(users)*0.1)]\n",
    "# select the data of that subsample of users\n",
    "tsSubData = tsData[tsData['user_id'].isin(users)]\n",
    "\n",
    "print('MAP@N (with N=',N,') for Collaborative Recommender: %s' % evaluate_topN(my_recsys.get_top_N_recommendations, \n",
    "                                                                               tsSubData,\n",
    "                                                                               N=N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
