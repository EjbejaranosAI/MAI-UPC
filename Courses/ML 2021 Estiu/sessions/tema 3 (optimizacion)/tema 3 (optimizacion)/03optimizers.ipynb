{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Lab creado por Margarita Geleta para el curso Introducción a Machine Learning JEDI, edición 2021`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41HBMLba5vly"
   },
   "source": [
    "# [III] Optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Derivar, es inevitable.** ¿Te acuerdas cómo se deriva una función?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La optimización es clave en técnicas de Machine Learning. Cuando definimos un modelo $f$ para aprender un patrón, también definimos una función de coste $J$ (una función de error). Esta función va definida por nuestros datos $x_1, x_2, ..., x_N$ que son fijos igual que las etiquetas $y_1, y_2, ..., y_N$. El modelo $f$ recibe como parámetero el dato $x_k$ y un parámetro $w$ (o una série de parámetros), y debe predecir, idealmente, $y_k$. La función de error calcula cuánto nos equivocamos, cuánto se desvia la predicción $f(x_k, w)$ de la realidad $y_k$.\n",
    "\n",
    "Comencemos por algo fácil. Supongamos que nuestra función de error es tan simple como la resta de $f(x_k, w)$ y $y_k$ al cuadrado (para que sea positivo):\n",
    "\n",
    "$J(x_k, w) = (f(x_k, w) - y_k)^2$ --> esto es el error para un punto en concreto\n",
    "\n",
    "$J(w) = \\sum_{k = 1}^N J(x_k, w) = \\sum_{k = 1}^N  (f(x_k, w) - y_k)^2$ --> el error del modelo es la suma de todos los errores puntuales\n",
    "\n",
    "Esta función de error se conoce por el nombre de **Error Cuadrático Medio** (https://en.wikipedia.org/wiki/Mean_squared_error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9cc1e5be7962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 200\n",
    "X = np.random.rand(N) * 10\n",
    "secreto = 2 * X - 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acabamos de crear 200 puntos $x_1, x_2, ..., x_{200}$. La función $2 * X - 4$ es la función que queremos modelar con Machine Learning y tiene dos parámetros $w$: $w_1 = 2$ y $w_2 = -4$. Estos son los parámetros que intentaremos determinar con el modelo. \n",
    "\n",
    "Recibimos las etiquetas con cierto error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.random.randn(N) * 2.25 # error de la distribución normal\n",
    "Y = 2 * X - 4 + error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, label = 'Datos reales')\n",
    "plt.plot(X, secreto, label = 'La función a modelar')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos la función de error ... ¿Cómo se define un sumatorio en Python?\n",
    "\n",
    "$J(w) = \\sum_{k = 1}^N  (f(x_k, w) - y_k)^2$\n",
    "\n",
    "$f(x_k, w) = x_k \\cdot w_1 + w_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(w):\n",
    "    return sum((w[0]*X[k] + w[1] - Y[k])**2 for k in range(0, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mínimo error que se puede cometer es pues ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J([2, -4]) # los parámetros OPTIMOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate que al variar un poco los parámetros, el error incrementa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J([2.01, -3.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comenzar el entrenamiento del modelo $f$ no tenemos ni idea de los valores $w_1$ y $w_2$. Por eso, escogeríamos unos valores al azar y a partir de ahí intentaremos optimizar para llegar al valor óptimo de la función de coste $J(w)$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([np.random.rand(2)]).T # Escojamos 2 valores aleatorios\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que no nos encontramos en el óptimo ($w_1 = 2, w_2 = -4$). ¿Cómo podemos bajar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; justify-content: center;\">\n",
    "    <img src=\"data/convex1.png\" alt=\"drawing\" style=\"height:500px;\"/>\n",
    "    <img src=\"data/convex2.png\" alt=\"drawing\" style=\"height:500px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, label = 'Datos reales')\n",
    "plt.plot(X, secreto, label = 'La función a modelar')\n",
    "plt.plot(X, w[0]*X + X[1], label = 'Inicialización aleatoria')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para descender de un punto a un mínimo en una función existen algoritmos de optimización que utilizan diferentes estrategias. El algoritmo de optimización más básico es el **Gradiente Descendiente** (https://en.wikipedia.org/wiki/Gradient_descent). La idea es la siguiente:\n",
    "\n",
    "- El gradiente de una función es la derivada de una función. El gradiente siempre apunta a la dirección de máximo crecimiento. \n",
    "- La dirección contraria llevaría a la dirección de decrecimiento.\n",
    "- Intentaremos bajar por la función de error por la dirección negativa del gradiente.\n",
    "\n",
    "Para aplicar este algoritmo necesitamos calcular antes la derivada de la función de coste:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(w) = \\sum_{k = 1}^N  (x_k \\cdot w_1 + w_2 - y_k)^2$\n",
    "\n",
    "$\\frac{\\partial f(x_k, w)}{\\partial w_1} = 1/N \\cdot \\sum_{k = 1}^N  2\\cdot x_k \\cdot (x_k \\cdot w_1 + w_2 - y_k)$\n",
    "\n",
    "$\\frac{\\partial f(x_k, w)}{\\partial w_2} = 1/N \\cdot \\sum_{k = 1}^N  2\\cdot (x_k \\cdot w_1 + w_2 - y_k)$\n",
    "\n",
    "$\\frac{\\partial J(w)}{\\partial w} = 2/N\\cdot \\left(\\sum_{k = 1}^N  x_k \\cdot (x_k \\cdot w_1 + w_2 - y_k) ,  \\sum_{k = 1}^N  (x_k \\cdot w_1 + w_2 - y_k) \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ(w):\n",
    "    gradient_w0 = (2 / N) * sum(X[k]*(w[0]*X[k] + w[1] - Y[k]) for k in range(0, N))\n",
    "    gradient_w1 = (2 / N) * sum((w[0]*X[k] + w[1] - Y[k]) for k in range(0, N))\n",
    "    return np.array([gradient_w0, gradient_w1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2411d0b4c688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdJ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-4a623b95a4f7>\u001b[0m in \u001b[0;36mdJ\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdJ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mgradient_w0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mgradient_w1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgradient_w0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_w1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N' is not defined"
     ]
    }
   ],
   "source": [
    "dJ([2, -4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos la líbreria con algoritmos de optimización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -i https://test.pypi.org/simple/ optrita==0.0.3\n",
    "\"\"\"\n",
    "Si no va, probar:\n",
    "import sys\n",
    "!{sys.executable} -m pip install -i https://test.pypi.org/simple/ optrita==0.0.3\n",
    "\"\"\"\n",
    "import optrita as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al algoritmo le pasamos la siguiente información:\n",
    "- El punto inicial $w$.\n",
    "- La función de error $J$.\n",
    "- La derivada de la función de error $\\frac{\\partial J(w)}{\\partial w}$.\n",
    "- Parámetros de optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLS_params = dict(alpha_max = 2.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.5,\n",
    "                  c1 = 0.01, c2 = 0.45,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "\n",
    "Xk_GM, info_GM = opt.GM(w, J, dJ, BLS_params, eps = 1e-6, kmax = 1500, precision = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xk_GM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como que la función de coste está definida solamente por dos parámetros, la función de coste se puede visualizar en un plot tridimensional. Podemos visualizar el descenso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.contour_map(J, Xk_GM[0], Xk_GM[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos observar con qué rapidez disminuye la norma del gradiente (cómo más próxima sea a 0, más cerca estamos a un punto crítico). A veces es útil verlo en escala logarítmica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(info_GM['||g(x)||'])\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos ver los diferentes modelos que se han ido generando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, label = 'Datos reales')\n",
    "for i in range(0, len(Xk_GM[0]), 10):\n",
    "    plt.plot(X, Xk_GM[0][i]*X + Xk_GM[1][i], label = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, label = 'Datos reales')\n",
    "plt.plot(X, secreto, label = 'La función a modelar')\n",
    "plt.plot(X, Xk_GM[0][-1]*X + Xk_GM[1][-1], label = 'Nuestro modelo')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la base de Machine Learning:\n",
    "- Se define una función de coste (función de error) según el problema.\n",
    "- Se escoge un algoritmo de optimización.\n",
    "- Se optimiza con el algoritmo en la función de coste para encontrar los parámetros óptimos.\n",
    "\n",
    "Cuidado!\n",
    "- No hay que optimizar demasiado porque sino podemos hacer overfitting.\n",
    "- Optimizando poco, hacemos underfitting.\n",
    "- Hemos visto una función de coste muy simple. En la vida real, tenemos muchos paráemtros $w$, las funciones no son bidimensionales, tienen muchas dimensiones, son muy complejas, tienen muchos puntos críticos (aparte de mínimos locales, también puntos de silla), no lo podemos visualizar y es más difícil optimizar.\n",
    "\n",
    "¿Qué buscamos?\n",
    "- Buscamos algoritmos de optimización de vayan muy rápidos (--> convergencia local).\n",
    "- Buscamos algoritmos de optimización que lleguen a un mínimo local (--> convergencia global)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GM (Gradient Descent) es el algoritmo de optimización más básico. CGM (Conjugate Gradient Descent) aparte del gradiente de la iteración actual, también utiliza el gradiente de la iteración anterior y la dirección de descenso se calcula a partir de una combinación lineal de gradientes. Vamos a probar - Hay dos variaciones de CGM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xk_CGM, info_CGM = opt.CGM(w, J, dJ, BLS_params, iCG = 1, iRC = 1, eps = 1e-6, kmax = 1500, precision = 6,nu = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.contour_map(J, Xk_CGM[0], Xk_CGM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xk_CGM, info_CGM = opt.CGM(w, J, dJ, BLS_params, iCG = 2, iRC = 1, eps = 1e-6, kmax = 1500, precision = 6,nu = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.contour_map(J, Xk_CGM[0], Xk_CGM[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mucho más rápido!\n",
    "\n",
    "BFGS (Broyden-Fletcher-Goldfarb-Shanno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xk_BFGS, info_BFGS = opt.BFGS(w, J, dJ, BLS_params, eps = 1e-6, kmax = 1500, precision = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.contour_map(J, Xk_BFGS[0], Xk_BFGS[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(info_GM['||g(x)||'], label = 'GM')\n",
    "plt.plot(info_CGM['||g(x)||'], label = 'CGM')\n",
    "plt.plot(info_BFGS['||g(x)||'], label = 'BFGS')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora solo hemos utilizado información de la primera derivada (el gradiente).\n",
    "También hay algoritmos de optimización que utilizan información de la segunda derivada (la Hessiana), que aporta información sobre la curvatura de la función y ayuda a encontrar el camino de descenso más rápido. El inconveniente de estos algoritmos es que se tiene que calcular la Hessiana (derivada de derivada)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si te interesa la implementación de estos algoritmos de optimización, la puedes encontrar en mi GitHub:\n",
    "https://github.com/margaritageleta/optrita/blob/master/optrita/linesearch/base.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a experimentar con los algoritmos de optimización "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_otVqBUv4b9B"
   },
   "source": [
    "### Ejemplo 1\n",
    "\n",
    "$f(x,y) = x^2 + y^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; justify-content: center; align-items: center;\">\n",
    "    <img src=\"data/convex.png\" alt=\"drawing\" style=\"height:500px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos la función, su gradiente y un punto inicial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 753,
     "status": "ok",
     "timestamp": 1572794841287,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "TjKhWVwmWJQz",
    "outputId": "4dd71a78-5d79-4a31-ed7d-60abdf1fd568"
   },
   "outputs": [],
   "source": [
    "# Problem\n",
    "def f(x): return x[0]**2 + x[1]**2 \n",
    "def g(x): return np.array([2*x[0], 2*x[1]])\n",
    "\n",
    "x = np.array([[50, -50]]).T\n",
    "\n",
    "BLS_params = dict(alpha_max = 2.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.5,\n",
    "                  c1 = 0.01, c2 = 0.45,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "\n",
    "Xk_GM, info_GM = opt.GM(x, f, g, BLS_params, eps = 1e-6, kmax = 1500, precision = 6)\n",
    "Xk_CGM, info_CGM = opt.CGM(x, f, g, BLS_params, iCG = 1, iRC = 1, eps = 1e-6, kmax = 1500, precision = 6,nu = 0.1)\n",
    "Xk_BFGS, info_BFGS = opt.BFGS(x, f, g, BLS_params, eps = 1e-6, kmax = 1500, precision = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1572794853168,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "lZc2GZnzydlm",
    "outputId": "83779994-0161-4261-d8e8-3f1d2403aacf"
   },
   "outputs": [],
   "source": [
    "opt.contour_map(f, Xk_GM[0], Xk_GM[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función es muy simple, los algoritmos encuentran el óptimo en una iteración. Hasta ahora hemos hablado sobre cómo se escoge la dirección del descenso (el gradiente negativo, o la combinación de gradientes, etc.) pero no hemos hablado sobre la \"longitud\" de paso en el momento de bajar. Es aquí donde entra en juego el paquete de parámetros de optimización `BLS_params`. Más adelante, vamos a ver funciones de error más complicadas y el efecto que tienen estos parámetros en la optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VHNpGQew4gaB"
   },
   "source": [
    "### Ejemplo 2\n",
    "\n",
    "$f(x,y) = -x^3 - \\frac{1}{2}x^2 + \\frac{1}{2}y^2 + \\frac{1}{4}xy + x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1572794904621,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "3xbRSykj-eqN",
    "outputId": "4011faa5-b736-4d38-fa1f-b76426cfc90a"
   },
   "outputs": [],
   "source": [
    "# Problem\n",
    "def f(x): return -x[0]**3 - (1/2)*x[0]**2 + (1/2)*x[1]**2 + (1/4)*x[0]*x[1] + x[0]\n",
    "def g(x): return np.array([-3*x[0]**2 - x[0] + (1/4)*x[1] + 1,\n",
    "                           x[1] + (1/4)*x[0]])\n",
    "\n",
    "x = np.array([[0.4, 0.6]]).T\n",
    "\n",
    "BLS_params = dict(alpha_max = 2.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.5,\n",
    "                  c1 = 0.01, c2 = 0.45,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "\n",
    "Xk_GM, info_GM = opt.GM(x, f, g, BLS_params, eps = 1e-6, kmax = 1500, precision = 6)\n",
    "Xk_CGM, info_CGM = opt.CGM(x, f, g, BLS_params, iCG = 2, iRC = 1, eps = 1e-6, kmax = 1500, precision = 6,nu = 0.1)\n",
    "Xk_BFGS, info_BFGS = opt.BFGS(x, f, g, BLS_params, eps = 1e-6, kmax = 1500, precision = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1572794972010,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "p7n1qHD64qkf",
    "outputId": "18a48438-d4f8-44ef-b120-061ab804559f"
   },
   "outputs": [],
   "source": [
    "plt.plot(info_GM[\"||g(x)||\"], label = \"GM\")\n",
    "plt.plot(info_CGM[\"||g(x)||\"], label = \"CGM\")\n",
    "plt.plot(info_BFGS[\"||g(x)||\"], label = \"BFGS\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1572794981797,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "Q6cnDsoQ4rMX",
    "outputId": "f6797593-851e-448a-986a-50d292c9e6df"
   },
   "outputs": [],
   "source": [
    "opt.contour_map(f, Xk_GM[0], Xk_GM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1572794990961,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "6NIEnsUT4thw",
    "outputId": "2e31b204-732c-4c6e-fb6d-0de13fed7900"
   },
   "outputs": [],
   "source": [
    "opt.contour_map(f, Xk_CGM[0], Xk_CGM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1160,
     "status": "ok",
     "timestamp": 1572794999457,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "GpO-6nQW4vqt",
    "outputId": "4c979a44-f886-4c13-d661-202005a95e71"
   },
   "outputs": [],
   "source": [
    "opt.contour_map(f, Xk_BFGS[0], Xk_BFGS[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependiendo del punto inicial, a veces encontramos soluciones diferentes ...\n",
    "Hay que ir con cuidado, a veces podemos caer a puntos de silla!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1983,
     "status": "ok",
     "timestamp": 1572795025326,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "Bm2aIbe74x2D",
    "outputId": "0adfaf59-cc52-411b-9e51-b0e9c3397b2e"
   },
   "outputs": [],
   "source": [
    "x = np.array([[0.1, 0.6]]).T\n",
    "Xk_GM, info_GM = opt.GM(x, f, g, BLS_params, eps = 1e-6, kmax = 1500, precision = 6)\n",
    "Xk_BFGS, info_BFGS = opt.BFGS(x, f, g, BLS_params, eps = 1e-6, kmax = 1500, precision = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1534,
     "status": "ok",
     "timestamp": 1572795032573,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "kVDHR4c943-n",
    "outputId": "a617a9ff-2b3c-4c2d-862b-1dd6b1250c83"
   },
   "outputs": [],
   "source": [
    "opt.contour_map(f, Xk_GM[0], Xk_GM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1142,
     "status": "ok",
     "timestamp": 1572795040664,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "UpKEgPRp45wT",
    "outputId": "66e495fd-4a63-4c77-ba8a-80a46d80b6d7"
   },
   "outputs": [],
   "source": [
    "opt.contour_map(f, Xk_BFGS[0], Xk_BFGS[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I52P6dOW5AEM"
   },
   "source": [
    "### Ejemplo 3\n",
    "\n",
    "Aquí tenemos un ejemplo de una función con 3 paráemtros (ya no la podemos visualizar, es 4D):\n",
    "\n",
    "$f(x_0,x_1,x_2) = (x_0 - 1)^2 + x_1^3 + x_2^2$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x_0} = 2(x_0 - 1) \\cdot 1$\n",
    ",$\\frac{\\partial f}{\\partial x_1} = 3x_1^2$\n",
    ",$\\frac{\\partial f}{\\partial x_2} = 2x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1087,
     "status": "ok",
     "timestamp": 1572795067702,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "9ZTU9GE1474F",
    "outputId": "3da9b796-061d-4f6b-ab59-6dc898630bd8"
   },
   "outputs": [],
   "source": [
    "# Problem\n",
    "def f(x): return (x[0] - 1)**2 + x[1]**3 + x[2]**2\n",
    "def g(x): return np.array([2*(x[0] - 1),\n",
    "                           3*x[1]**2,\n",
    "                           2*x[2]])\n",
    "\n",
    "x = np.array([[0.8, 0.1, 0.3]]).T\n",
    "\n",
    "BLS_params = dict(alpha_max = 2.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.4,\n",
    "                  c1 = 0.01, c2 = 0.45,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "Xk_GM, info_GM = opt.GM(x, f, g, BLS_params, eps = 1e-6, kmax = 1200, precision = 6)\n",
    "Xk_CGM, info_CGM = opt.CGM(x, f, g, BLS_params, iCG = 2, iRC = 1,  eps = 1e-6, kmax = 1200, precision = 6, nu = 9)\n",
    "Xk_BFGS, info_BFGS = opt.BFGS(x, f, g, BLS_params, eps = 1e-6, kmax = 1200, precision = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1701,
     "status": "ok",
     "timestamp": 1572795077116,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "fyHi4l5-5Cik",
    "outputId": "08c6c3da-0422-4552-ca60-8ed1c60aefef"
   },
   "outputs": [],
   "source": [
    "plt.plot(info_GM[\"||g(x)||\"], label = \"GM\")\n",
    "plt.plot(info_CGM[\"||g(x)||\"], label = \"CGM\")\n",
    "plt.plot(info_BFGS[\"||g(x)||\"], label = \"BFGS\")\n",
    "#plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Más ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BFGS ha demostrado ser mucho más rápido que CGM y GM en muchos casos. Pero no siempre es así.\n",
    "No hay que menospreciar GM, aunque sea el más simple de todos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2935,
     "status": "ok",
     "timestamp": 1572795165244,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "ob8HEKRh5XLI",
    "outputId": "046f45cd-49d4-4367-a574-6576fd829c42"
   },
   "outputs": [],
   "source": [
    "def f(x): return x[0]*np.exp(-x[0]**2-x[1]**2)\n",
    "def g(x): return np.array([np.exp(-x[0]**2-x[1]**2)*(1 - 2*x[0]**2),\n",
    "                           np.exp(-x[0]**2-x[1]**2)*(-2*x[0]*x[1])])\n",
    "x = np.array([[-1,1]]).T\n",
    "\n",
    "BLS_params = dict(alpha_max = 2.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.4,\n",
    "                  c1 = 0.01, c2 = 0.45,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "\n",
    "Xk_GM, info_GM = opt.GM(x, f, g, BLS_params, eps = 1e-6, kmax = 1500, precision = 6)\n",
    "Xk_CGM, info_CGM = opt.CGM(x, f, g, BLS_params, iCG = 1, iRC = 1, eps = 1e-6, kmax = 1500, precision = 6,nu = 0.1)\n",
    "Xk_BFGS, info_BFGS = opt.BFGS(x, f, g, BLS_params, eps = 1e-6, kmax = 1500, precision = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1006,
     "status": "ok",
     "timestamp": 1572795175023,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "ILvPjR6q5Z5y",
    "outputId": "8df8d9ab-1b24-4200-8c1f-b127b120fcb8"
   },
   "outputs": [],
   "source": [
    "opt.contour_map(f, Xk_GM[0], Xk_GM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1421,
     "status": "ok",
     "timestamp": 1572795185158,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "xdX-vm0F5cwM",
    "outputId": "00ed0bed-e223-48ca-ad0f-2288fcdf5752"
   },
   "outputs": [],
   "source": [
    "opt.contour_map(f, Xk_CGM[0], Xk_CGM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1228,
     "status": "ok",
     "timestamp": 1572795193252,
     "user": {
      "displayName": "Rita Geleta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBPwNQFHbggnshTIQw_mrDVDTXjk7x6lbmkAt_4=s64",
      "userId": "02951064353604132782"
     },
     "user_tz": -60
    },
    "id": "V9xo4Y7n5e0I",
    "outputId": "b0fdf462-e626-487e-e108-61ad6b8d602f"
   },
   "outputs": [],
   "source": [
    "opt.contour_map(f, Xk_BFGS[0], Xk_BFGS[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLS_params = dict(alpha_max = 2.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.9,\n",
    "                  c1 = 0.1, c2 = 0.8,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "Xk_CGM, info_CGM = opt.CGM(x, f, g, BLS_params, iCG = 1, iRC = 1, eps = 1e-6, kmax = 1500, precision = 6,nu = 0.1)\n",
    "opt.contour_map(f, Xk_CGM[0], Xk_CGM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLS_params = dict(alpha_max = 2.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.2,\n",
    "                  c1 = 0.1, c2 = 0.8,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "Xk_CGM, info_CGM = opt.CGM(x, f, g, BLS_params, iCG = 1, iRC = 1, eps = 1e-6, kmax = 1500, precision = 6,nu = 0.1)\n",
    "opt.contour_map(f, Xk_CGM[0], Xk_CGM[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; justify-content: center; align-items: center;\">\n",
    "    <img src=\"data/convex3.png\" alt=\"drawing\" style=\"height:200px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLS_params = dict(alpha_max = 2.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.9,\n",
    "                  c1 = 0.5, c2 = 0.8,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "Xk_CGM, info_CGM = opt.CGM(x, f, g, BLS_params, iCG = 1, iRC = 1, eps = 1e-6, kmax = 1500, precision = 6,nu = 0.1)\n",
    "opt.contour_map(f, Xk_CGM[0], Xk_CGM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLS_params = dict(alpha_max = 2.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.2,\n",
    "                  c1 = 0.01, c2 = 0.01,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "Xk_CGM, info_CGM = opt.CGM(x, f, g, BLS_params, iCG = 1, iRC = 1, eps = 1e-6, kmax = 1500, precision = 6,nu = 0.1)\n",
    "opt.contour_map(f, Xk_CGM[0], Xk_CGM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLS_params = dict(alpha_max = 2.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.01,\n",
    "                  c1 = 0.1, c2 = 0.99,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "Xk_CGM, info_CGM = opt.CGM(x, f, g, BLS_params, iCG = 1, iRC = 1, eps = 1e-6, kmax = 1500, precision = 6,nu = 0.1)\n",
    "opt.contour_map(f, Xk_CGM[0], Xk_CGM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLS_params = dict(alpha_max = 7.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.9,\n",
    "                  c1 = 0.1, c2 = 0.8,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "Xk_CGM, info_CGM = opt.CGM(x, f, g, BLS_params, iCG = 1, iRC = 1, eps = 1e-6, kmax = 1500, precision = 6,nu = 0.1)\n",
    "opt.contour_map(f, Xk_CGM[0], Xk_CGM[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLS_params = dict(alpha_max = 2.0,\n",
    "                  alpha_min = 1e-3,\n",
    "                  rho = 0.9,\n",
    "                  c1 = 0.1, c2 = 0.8,\n",
    "                  strong_wolfe = True, out = 0)\n",
    "Xk_CGM, info_CGM = opt.CGM(x, f, g, BLS_params, iCG = 2, # modificar\n",
    "                           iRC = 1, # modificar\n",
    "                           eps = 1e-6, kmax = 1500, \n",
    "                           precision = 6, \n",
    "                           nu = 3) # modificar\n",
    "opt.contour_map(f, Xk_CGM[0], Xk_CGM[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema: Minimizar el coste de una Máquina\n",
    "\n",
    "Supongamos que tienes una máquina que recibe energia $E$ para realizar una tarea. Tenemos un modo que ajusta la complejidad de las operaciones de la máquina. Para tener esta máquina en funcionamiento, hay unos costes. Queremos encontrar la complejidad y la energía para minimizar los costes.\n",
    "\n",
    "- $E$ es la Energía. (puede ser negativa!)\n",
    "- $K$ es la Complejidad del sistema en unidades secretas.\n",
    "- $cost(E, K)$ es el coste del trabajo.\n",
    "\n",
    "$cost(E, K) = K^2 + E^3 + 3\\cdot E \\cdot K$\n",
    "\n",
    "Encuentra los valores y decide qué algoritmo de optimización y qué parámetros proporcionan la convergencia más rápida y la solución óptima. \n",
    "\n",
    "> ¿Parece difícil?\n",
    "\n",
    "Pista + Instrucciones: \n",
    "- Define la función `def f(x): return` $K^2 + E^3 + 3\\cdot E \\cdot K$\n",
    "- Calcula la derivada y definela en `def g(x): return ...`\n",
    "- Escoge un punto inicial aleatorio, con `x = np.array([np.random.rand(2)]).T * -10`.\n",
    "- Juega con los parámetros `BLS_params`.\n",
    "- Decide el algoritmo de optimización.\n",
    "- Dibuja la direcciones de descenso.\n",
    "- Dibuja la disminución de la norma del gradiente.\n",
    "\n",
    "> Q - A\n",
    "- ¿Me da infinito? Mala inicialización.\n",
    "- ¿Llega al número máximo de iteraciones? Está haciendo pasitos pequeños. Intenta modificar la `rho`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem\n",
    "def f(x): return ...\n",
    "def g(x): return ...\n",
    "\n",
    "x = np.array([np.random.rand(2)]).T * -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLS_params = dict(alpha_max = 2.0, \n",
    "                 alpha_min = 1e-3, # NO modificar\n",
    "                 rho = 0.3, # modificar\n",
    "                 c1 = 0.8, c2 = 0.4, # modificar\n",
    "                 strong_wolfe = True, out = 0) # NO modificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "optimizers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
